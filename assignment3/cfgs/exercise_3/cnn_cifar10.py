from functools import partial

import torch
import torch.nn as nn

from src.data_loaders.data_modules import CIFAR10DataModule
from src.trainers.cnn_trainer import CNNTrainer
from src.models.cnn.model import ConvNet
from src.models.cnn.metric import TopKAccuracy

q1_experiment = dict(
    name = 'CIFAR10_CNN_WO_BN',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = None,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)


#########  TODO #####################################################
#  You would need to create the following config dictionaries       #
#  to use them for different parts of Q2 and Q3.                    #
#  Feel free to define more config files and dictionaries if needed.#
#  But make sure you have a separate config for every question so   #
#  that we can use them for grading the assignment.                 #
#####################################################################
q2a_normalization_experiment = dict(
    name = 'CIFAR10_CNN_BN',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q2c_earlystop_experiment = dict(
    name = 'CIFAR10_CNN_EARLY_STOP',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "max eval_top1",
        early_stop = 4,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3a_aug_flip = dict(
    name = 'CIFAR10_CNN_aug_flip',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10_WithFlip',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 25,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)


q3a_aug_crop_flip = dict(
    name = 'CIFAR10_CNN_aug_crop_flip',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10_WithCropFlip',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 25,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3a_aug_crop_flip_rotate = dict(
    name = 'CIFAR10_CNN_aug_crop_flip_rotate',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10_WithCropFlipRotate',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 25,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3a_aug_jitter = dict(
    name = 'CIFAR10_CNN_aug_jitter',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10_WithColorJitter',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 25,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3a_aug_aggressive = dict(
    name = 'CIFAR10_CNN_aug_aggressive',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.4,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10_Aggressive',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 25,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)


q3b_dropout0 = dict(
    name = 'CIFAR10_drop_0',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3b_dropout2 = dict(
    name = 'CIFAR10_drop_2',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.2,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3b_dropout6 = dict(
    name = 'CIFAR10_drop_6',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.6,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

q3b_dropout9 = dict(
    name = 'CIFAR10_drop_9',

    model_arch = ConvNet,
    model_args = dict(
        input_size = 3,
        num_classes = 10,
        hidden_layers = [128, 512, 512, 512, 512, 512],
        activation = nn.ReLU(),
        norm_layer = nn.BatchNorm2d,
        drop_prob = 0.9,
    ),

    datamodule = CIFAR10DataModule,
    data_args = dict(
        data_dir = "data/exercise-2", # You may need to change this for Colab.
        transform_preset = 'CIFAR10',
        batch_size = 200,
        shuffle = True,
        heldout_split = 0.1,
        num_workers = 6,
    ),

    optimizer = partial(
        torch.optim.Adam,
        lr=0.002, weight_decay=0.001, amsgrad=True,
    ),
    lr_scheduler = partial(
        torch.optim.lr_scheduler.StepLR,
        step_size=5, gamma=0.8
    ),

    criterion = nn.CrossEntropyLoss,
    criterion_args = dict(),

    metrics=dict(
        top1 = TopKAccuracy(k=1),
        top5 = TopKAccuracy(k=5),
    ),

    trainer_module = CNNTrainer,
    trainer_config = dict(
        n_gpu = 1,
        epochs = 10,
        eval_period = 1,
        save_dir = "Saved",
        save_period = 10,
        monitor = "off",
        early_stop = 0,

        log_step = 100,
        tensorboard=True,
        wandb=False,
    ),

)

# define more config dictionaries if needed...