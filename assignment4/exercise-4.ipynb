{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter your team number and members here:\n",
    "\n",
    "Team NR: \n",
    "\n",
    "Team Members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = './' \n",
    "\n",
    "\n",
    "import sys\n",
    "from os.path import join as ospj\n",
    "sys.path.append(ospj(PROJECT_ROOT, 'src'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of assignment 4 we try to\n",
    "1. implement the forward pass of ViT\n",
    "2. visualize the attention weights for the final CLS Token.\n",
    "\n",
    "\n",
    "For evaluation and visualization, we use an 8-class subset (out of 1000) of Imagenet-V2. We use a subset, simply just so that you can run the cells faster.\n",
    "The model is pre-trained and you just need to download the weights.\n",
    "\n",
    "Furthermore, most of the relevant modules for ViT, such as the positional embedding, the linear projection, MLP Block, and Multi-head attention are all already implemented. What you mainly have to do is to plug these modules together based on what you've seen in lecture 7 (the ViT slides).\n",
    "\n",
    "You probably shouldn't need GPU for running this assignment, as there is no training and the dataset is very small and every cell. But of course feel free to change the notebook so that it works on GPU if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm \n",
    "\n",
    "from models.vit.model import VisionTransformer\n",
    "from src.utils.imagenet_names import name_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading pre-trained weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Download the weights from `https://download.pytorch.org/models/vit_b_16-c867db91.pth` and place it under `saved/models/vit/vit.pth`. The file size is 330M.\n",
    "\n",
    "\n",
    "or you can just download and place it by running the cell below (tested on Linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\"\n",
    "\n",
    "# Change the relative paths below to absolute paths if running on Colab\n",
    "!mkdir -p ../../saved/models/vit\n",
    "!mv vit_b_16-c867db91.pth ../../saved/models/vit/vit.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: ViT Inference (15 points)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part you need to complete the missing parts of the ViT model. \n",
    "\n",
    "<img src=\"./data/exercise-4/ViT.png\" width=1400 />\n",
    "\n",
    "There are overall 7 `TODO Q1`s to be implemented for this question. \n",
    "\n",
    "You can of course define new attributes in the classes if you find it necessary. However, you cannot define new modules *with parameters* or change the name modules with parameters.\n",
    "Otherwise, the `load_state_dict` will fail to match the weight values in the file with the code. Also changing the configs such as number of layers or dimensions would cause the same mismatch in loading weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(ospj(PROJECT_ROOT,'saved/models/vit/vit.pth'))\n",
    "\n",
    "model = VisionTransformer(\n",
    "            image_size=224, # Input image size (width and height)\n",
    "            patch_size=16,  # Image broken into (16 x 16) non-overlaping batches\n",
    "            num_layers=12,  # Number of blocks in the Encoder\n",
    "            num_heads=12,   # Number of heads in each Multi-\"head\" attention\n",
    "            hidden_dim=768, # Token size (length of a single token)\n",
    "            mlp_dim=3072,   # Hidden layer size of each MLP layer\n",
    "        )\n",
    "\n",
    "model.load_state_dict(weights, strict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using only 8 classes of Imagenet-V2 for this assignment. Feel free to checkout the entire dataset (https://github.com/modestyachts/ImageNetV2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're just using a subset of classes, the labels that ImageFolder generates (starting from 0 to 7) \n",
    "# should be converted back to correct (0 to 999) numbers.\n",
    "def label_convert(idx):\n",
    "    class_subset = sorted([19, 330, 466, 558, 672, 716, 755, 986,])\n",
    "    return class_subset[idx] # e.g. 0 -> 19, 2 -> 466\n",
    "\n",
    "IMN_MEAN, IMN_STD = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "t = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Normalize(mean=IMN_MEAN, std=IMN_STD) \n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "        root=ospj(PROJECT_ROOT, 'data/exercise-4/IMN-Subset'),\n",
    "        transform=t, \n",
    "        target_transform=label_convert\n",
    "    )\n",
    "print(f\"Test set is created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "                dataset=dataset,\n",
    "                drop_last=False,\n",
    "                shuffle=False, \n",
    "                batch_size=16, # Change the batch size if the model doesn't fit into memory.\n",
    "                num_workers=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "total_cnt = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, targets) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        out, attention_weights = model(images)\n",
    "        pred = out.argmax(dim=1)\n",
    "\n",
    "        num_correct += (pred == targets).sum()\n",
    "        total_cnt += len(images)\n",
    "\n",
    "top1_acc = 100*num_correct/total_cnt\n",
    "\n",
    "print(f\"Top-1 Acc on Test set is {100 * num_correct/total_cnt}\")\n",
    "# Should be very close to 83.75"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Attention (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just used to reverse the normalization done in the Dataset transforms.\n",
    "def unnormalize(img_tensor):\n",
    "    assert img_tensor.ndim == 3 and img_tensor.shape[0] == 3\n",
    "    img_tensor = img_tensor * torch.Tensor(IMN_STD)[:, None, None]\n",
    "    img_tensor = img_tensor + torch.Tensor(IMN_MEAN)[:, None, None]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, label = dataset[42]\n",
    "plt.title(f\"GT {label} : {name_map[label]}\")\n",
    "plt.imshow(unnormalize(x).moveaxis(0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, attention_weights = model(x[None])\n",
    "\n",
    "pred = output.argmax(dim=1).item()\n",
    "print(f\"Predicted class {pred} : {name_map[pred]}\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(unnormalize(x).moveaxis(0, -1))\n",
    "plt.title(f\"GT {label} : {name_map[label]}\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(attention_weights[0], cmap='gray')\n",
    "plt.title(f\"Pred {pred} : {name_map[pred]}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please briefly answer the following based on your intuition. (5 points)\n",
    "1. Why does the attention map have lower resolution?\n",
    "2. Browse other samples of the dataset (simply change the idx above or add extra cells below). Is the attention map of final CLS token always localized on the object? Briefly explain your intuition behind what you see.\n",
    "3. As mentioned above, we're only visualizing the attention weights of the CLS token at the last layer.\n",
    "   \n",
    "   Can you think of any problems regarding this? \n",
    "\n",
    "   Can you come up with some other way of looking into a decision made by a transformer model?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
