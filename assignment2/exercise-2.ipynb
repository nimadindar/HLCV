{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "# This allows you to edit the imported modules (e.g code in other files) without having to restart the kernel.\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks have shown staggering performances in various learning tasks, including computer vision, natural language processing, and sound processing. They have made the model design more flexible by enabling end-to-end training.\n",
    "\n",
    "In this exercise, we get to have a first hands-on experience with neural network training. Many frameworks (e.g., PyTorch, Tensorflow, Caffe) allow easy usage of deep neural networks without precise knowledge of the inner workings of backpropagation and gradient descent algorithms. While these are very useful tools, it is important to get a good understanding of how to implement basic network training from scratch before using these libraries to speed up the process. For this purpose, we will implement a simple two-layer neural network and its training algorithm based on back-propagation using only basic matrix operations in questions 1 to 3. In question 4, we will use a popular deep learning library, PyTorch, to do the same and understand the advantages offered by using such tools.\n",
    "\n",
    "As a benchmark to test our models, we consider an image classification task using the widely used CIFAR-10 dataset. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to code and train a parametrized model for classifying those images. This involves\n",
    "\n",
    "- Implementing the feedforward model (Question 1).\n",
    "- Implementing the backpropagation algorithm (gradient computation) (Question 2).\n",
    "- Training the model using stochastic gradient descent and improving the model training with better hyper-parameters (Question 3).\n",
    "- Using the PyTorch Library to implement the above and experiment with deeper networks (Question 4).\n",
    "\n",
    "A note on notation: Throughout the exercise, notation $v_i$ is used to denote the $i$-th element of vector $v$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Implementing the feedforward model (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will implement a two-layered neural network architecture and the loss function to train it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failed to load image from ./data/exercise-2/fig1.png You can view it manually](./data/exercise-2/fig1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model architecture.** Our architecture is shown in Fig.1. It has an input layer and two model layers – a hidden and an output layer. We start with randomly generated toy inputs of four dimensions and the number of classes K = 3 to build our model in Q1 and Q2, and in Q3 use images from the CIFAR-10 dataset to test our model on a real-world task. Hence input layer is 4-dimensional for now.\n",
    "\n",
    "In the hidden layer, there are 10 units. The input layer and the hidden\n",
    "layer are connected via linear weighting matrix $W^{(1)}\\in\\mathbb{R}^{10\\times\n",
    "4}$ and the bias term $b^{(1)}\\in\\mathbb{R}^{10}$. The parameters $W^{(1)}$\n",
    "and $b^{(1)}$ are to be learnt later on. A linear operation is performed,\n",
    "$W^{(1)}x+b^{(1)}$, resulting in a 10 dimensional vector $z^{(2)}$. It is then\n",
    "followed by a relu non-linear activation $\\phi$, applied element-wise on each\n",
    "unit, resulting in the activations $a^{(2)} = \\phi(z^{(2)})$. Relu function has\n",
    "the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(u) =  \\begin{cases}\n",
    "      u, & \\text{if}\\ u\\geq0 \\\\\n",
    "      0, & \\text{if}\\ u <0\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "A similar linear operation is performed on $a^{(2)}$, resulting in $z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$, where $W^{(2)}\\in\\mathbb{R}^{3\\times 10}$ and $b^{(2)}\\in\\mathbb{R}^{3}$; it is followed by the softmax activation to result in $a^{(3)}=\\psi(z^{(3)})$. The softmax function is defined by:\n",
    "\\begin{equation}\n",
    "\\psi(u)_i =  \\frac{\\exp^{u_i}}{\\sum_j{\\exp^{u_j}}} \n",
    "\\end{equation}\n",
    "\n",
    "The final functional form of our model is thus defined by\n",
    "\n",
    "\\begin{align*}\n",
    "a^{(1)} &= x \\\\ \\\n",
    "z^{(2)} &= W^{(1)}a^{(1)}+b^{(1)} \\\\\n",
    "a^{(2)} &= \\phi(z^{(2)}) \\\\\n",
    "z^{(3)} &= W^{(2)}a^{(2)}+b^{(2)} \\\\\n",
    "f_\\theta(x) := a^{(3)} &= \\psi(z^{(3)}),\n",
    "\\end{align*}\n",
    "\n",
    "which takes a flattened 4 dimensional vector as input and outputs a $3$ dimensional vector, each entry in the output $f_k(x)$ representing the probability of image $x$ corresponding to the class $k$. We summarily indicate all the network parameters by $\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation.** We are now ready to implement the feedforward neural network.\n",
    "\n",
    "a) Implement the feedforward model. Verify that the scores you generate for the toy inputs match the correct scores. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.twolayernet.model as module_twolayernet\n",
    "from utils.utils import seed_everything, init_toy_data, rel_error\n",
    "from utils.gradient_check import eval_numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[0.3644621  0.22911264 0.40642526]\n",
      " [0.47590629 0.17217039 0.35192332]\n",
      " [0.43035767 0.26164229 0.30800004]\n",
      " [0.41583127 0.2983228  0.28584593]\n",
      " [0.36328815 0.32279939 0.31391246]]\n",
      "\n",
      "Correct scores:\n",
      "[[0.3644621  0.22911264 0.40642526]\n",
      " [0.47590629 0.17217039 0.35192332]\n",
      " [0.43035767 0.26164229 0.30800004]\n",
      " [0.41583127 0.2983228  0.28584593]\n",
      " [0.36328815 0.32279939 0.31391246]]\n",
      "Difference between your scores and correct scores:\n",
      "2.9173411603133914e-08\n"
     ]
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "# You would need to implement the forward() pass of the TwoLayerNetv1 module.\n",
    "net = module_twolayernet.TwoLayerNetv1(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
    "X, y = init_toy_data(num_inputs, input_size)\n",
    "\n",
    "scores = net.forward(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('Correct scores:')\n",
    "correct_scores = np.asarray([\n",
    " [0.36446210, 0.22911264, 0.40642526],\n",
    " [0.47590629, 0.17217039, 0.35192332],\n",
    " [0.43035767, 0.26164229, 0.30800004],\n",
    " [0.41583127, 0.29832280, 0.28584593],\n",
    " [0.36328815, 0.32279939, 0.31391246]])\n",
    "print(correct_scores)\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))\n",
    "\n",
    "assert math.isclose(np.sum(np.abs(scores - correct_scores)), 0,  abs_tol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We later guide the neural network parameters\n",
    "$\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$ to fit the given data and label\n",
    "pairs. We do so by minimising the loss function. A popular choice of the loss\n",
    "function for training neural networks for multi-class classification is the\n",
    "cross-entropy loss. For a single input sample $x_i$, with label $y_i$, the loss\n",
    "function is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta, x_i, y_i) &= -\\log{P(Y=y_i,X=x_i)} \\\\\n",
    "                    &= -\\log{f_\\theta(x_i)_{y_i}} \\\\\n",
    "                    &= -\\log{\\psi(z^{(3)})_{y_i}} \\\\\n",
    "J(\\theta, x_i, y_i) &= -\\log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum^K_j{\\exp^{z^{(3)}_j}}}\\right]\n",
    "\\end{align}\n",
    "\n",
    "Averaging over the whole training set, we get \n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta,\\{x_i,y_i\\}_{i=1}^{N}) = \\frac{1}{N} \\sum_{i=1}^N \n",
    "-log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum^K_j{\\exp^{z^{(3)}_j}}}\\right],\n",
    "\\end{align}\n",
    "\n",
    "where $K$ is the number of classes. Note that if the model has perfectly fitted\n",
    "to the data (i.e. $f_\\theta^k(x_i)=1$ whenever $x_i$ belongs to class $k$ and 0\n",
    "otherwise), then $J$ attains the minimum of $0$. \n",
    "\n",
    "\n",
    "Apart from trying to correctly predict the label, we have to prevent\n",
    "overfitting the model to the current training data.  This is done by encoding\n",
    "our prior belief that the correct model should be simple (Occam's razor); we\n",
    "add an $L_2$ regularisation term over the model parameters $\\theta$.\n",
    "Specifically, the loss function is defined by:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{J}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N\n",
    "-log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum^K_j{\\exp^{z^{(3)}_j}}}\\right]\n",
    "+\\lambda \\left(||W^{(1)}||_2^2 + ||W^{(2)}||_2^2 \\right),\n",
    "\\end{align}\n",
    "\n",
    "where $||\\cdot||_2^2$ is the squared $L_2$ norm. For example,\n",
    "\n",
    "\\begin{align}\n",
    "||W^{(1)}||_2^2 = \n",
    "\\sum_{p=1}^{10} \\sum_{q=1}^{4} W_{pq}^{(1)2}\n",
    "\\end{align}\n",
    "\n",
    "By changing the value of $\\lambda$ it is possible to give weights to your prior belief on the degree of simplicity (regularity) of the true model. \n",
    "\n",
    "Implement the final loss function and let it return the loss value. Verify the code by\n",
    "running and matching the output cost $1.30378789133$. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "1.794120407794253e-13\n"
     ]
    }
   ],
   "source": [
    "net_v2 = module_twolayernet.TwoLayerNetv2(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1) \n",
    "loss = net_v2.compute_loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133 # check this number with your implementation\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n",
    "\n",
    "assert math.isclose(np.sum(np.abs(loss - correct_loss)), 0,  abs_tol=1e-6), 'The error with respect to the correct value is too high'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Backpropagation (15 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model by solving\n",
    "\\begin{align}\n",
    "\\underset{\\theta}{\\min} \\,\\,  \\tilde{J}(\\theta)\n",
    "\\end{align}\n",
    "via stochastic gradient descent. Therefore, We need an efficient computation of the gradients $\\nabla_\\theta \\tilde{J}(\\theta)$. We use backpropagation of top layer error signals to the parameters $\\theta$ at different layers.\n",
    "\n",
    "In this question, you will be required to implement the backpropagation algorithm yourself from pseudocode. We will give a high-level description of what is happening in each line.\n",
    "\n",
    "For those who are interested in the robust derivation of the algorithm, we include the optional exercise on the derivation of the backpropagation algorithm. A piece of prior knowledge of standard vector calculus, including the chain rule, would be helpful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation.** The backpropagation algorithm is simply a sequential application of the chain rule. It is applicable to any (sub-) differentiable model that is a composition of simple building blocks. In this exercise, we focus on the architecture with stacked layers of linear transformation + relu non-linear activation.\n",
    "\n",
    "The intuition behind the backpropagation algorithm is as follows. Given a training example $(x, y)$, we first run the feedforward to compute all the activations throughout the network, including the output value of the model $f_\\theta(x)$ and the loss $J$. Then, for each parameter in the model, we want to compute the effect that parameter has on the loss. This is done by computing the derivatives of the loss w.r.t for each model parameter.\n",
    "\n",
    "The backpropagation algorithm is performed from the top of the network (loss layer) to the bottom. It sequentially computes the gradient of the loss function with respect to each layer's activations and parameters.\n",
    "\n",
    "Let’s start by deriving the gradients of the un-regularized loss function w.r.t final layer activations $z^{(3)}$. We will then use this in the chain rule to compute analytical expressions for gradients of all the model parameters.\n",
    "\n",
    "(a) Verify that the loss function (in Q1) has the gradient w.r.t $z^{(3)}$ as below.\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\left(\\psi(z^{(3)}) - \\Delta\\right), \n",
    "\\end{equation}\n",
    "where $\\Delta$ is a matrix of $N\\times K$ dimensions with \n",
    "\\begin{align}\n",
    "        \\Delta_{ij} = 1, & \\text{if}\\ y_i =j \\\\\n",
    "                0, & \\text{otherwise}\n",
    "\\end{align}\n",
    "and $z^{(3)}$ here refers to all the activations of the training set.\n",
    "\\begin{align}\n",
    "        z^{(3)} \\in \\mathbb R^{N \\times K}\n",
    "\\end{align}\n",
    "(please write your answer in the block below, or attach an image in the same cell, 2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Please put your answer here.]\n",
    "We first compute the derivative $\\frac{\\partial \\psi(z^{(3)})_{y_i}}{\\partial z^{(3)}}$. Depending on which class of $z^{(3)}$ we are computing the derivative with, we get two derivatives (i) with respect to the correct class and (2) with respect to the incorrect class. \n",
    "\n",
    "(i) Computing the derivative with respect to the correct class. Let the correct class be $y_i$, and let $v = z_{y_i}^{(3)}$. Let $c = \\sum_{j\\neq i} \\exp^{z_{y_j}^{(3)}}$. Notice that $\\exp^v + c = \\sum_j \\exp^{z_{y_j}^{(3)}}$ which is the denominator of $\\psi(z^{(3)})$.Our derivative simplifies to: \n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\psi(z^{(3)})_{y_i}}{\\partial z_{y_i}^{(3)}} &= \\frac{\\partial ({\\exp^{v}}/{v + \\sum_{j \\neq i}{\\exp^{z_{y_j}^{(3)}}}})}{\\partial v} & \\text{unfolding definition}\\\\\n",
    "                                                          &= \\frac{(\\exp^v + c)(\\exp^v) - (\\exp^v)(\\exp^v)}{(\\exp^v + c)^2} & \\text{quotient rule}\\\\\n",
    "                                                          &= \\frac{\\exp^v}{\\exp^v + c}*\\frac{c}{\\exp^v + c} & \\text{simplification}\\\\\n",
    "                                                          &= \\psi(z^{(3)})_{y_i}*(1 - \\psi(z^{(3)})_{y_i}) & \\text{ By simplifying the second fraction with $c = \\exp^v + c - \\exp^v$}\n",
    "\\end{align*}\n",
    "\n",
    "(ii) Computing the derivative with respect to the incorrect class. Let the correct class be $y_i$ and the incorrect class with respect to which we are computing the derivative be $y_k$. Let $c_1 = \\exp^{z_{y_i}^{(3)}}$ and $c_2 = \\sum_{j\\neq k} \\exp^{z_{y_j}^{(3)}}$. Our derivative simplifies to:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\psi(z^{(3)})_{y_i}}{\\partial z_{y_k}^{(3)}} &= \\frac{\\partial (c_1/{\\exp^v + c_2})}{\\partial v} & \\text{unfolding definition}\\\\\n",
    "                                                          &= \\frac{-c_1*\\exp^v}{(\\exp^v + c_2)^2} & \\text{chain rule with $\\exp^v + c_2$}\\\\\n",
    "                                                          &= -\\psi(z^{(3)})_{y_i}*(\\psi(z^{(3)})_{y_k}) & \\text{Simplification}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Now, we can compute the partial derivative for the loss function. \n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial z^{(3)}} &= \\frac{-1}{N*\\psi(z^{(3)})_{y_i}}*\\frac{\\partial \\psi(z^{(3)})_{y_i}}{\\partial z^{(3)}} & \\text{ chain rule }\\\\\n",
    "                                        &= \\frac{-1}{N}* \\begin{bmatrix} \\begin{cases} psi(z^{(3)})_{y_i}*(1 - \\psi(z^{(3)})_{y_i}) & \\text{if } y_i = j \\\\ -\\psi(z^{(3)})_{y_i}*(\\psi(z^{(3)})_{y_k}) & \\text{otherwise}\\end{cases}\\end{bmatrix} & \\text{Based on (i) and (ii)}\\\\\n",
    "                                        &= \\frac{1}{N}* \\begin{bmatrix} \\begin{cases} (\\psi(z^{(3)})_{y_i} - 1) & \\text{if } y_i = j \\\\ (\\psi(z^{(3)})_{y_k}) & \\text{otherwise}\\end{cases}\\end{bmatrix}  & \\text{On simplification}\\\\\n",
    "                                        &= \\frac{1}{N}*(\\psi(z^{(3)}) - \\Delta)\n",
    "\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) To compute the effect of the weight matrix $W^{(2)}$ on the loss (in Q1) incurred by the network, we compute the\n",
    "partial derivatives of the loss function with respect to $W^{2}$.  This is done\n",
    "by applying the chain rule. Verify that the partial derivative of the loss w.r.t $W^{(2)}$ is  \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(2)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) &= \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta) a^{(2)^T}\n",
    "\\end{align}\n",
    "\n",
    "Similarly, verify that the regularized loss has the derivatives\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta) a^{(2)^T} + 2\\lambda W^{(2)}\n",
    "\\end{align}\n",
    "\n",
    "(please write your answer in the block below, or attach an image in the same cell, 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "$z^{(3)} = W^{(2)}\\cdot a^{(2)} + b^{(2)}$. If we take a partial derivative wrt $W^{(2)}$, then $a$ and $b$ are constants so the derivative is obvious. Similarly, the derivative wrt the regularized loss is obvious, as we need to add the derivative wrt the L2-regularization($\\lambda*(\\sum_{i, j} W_{ij}^{(1)2} + \\sum_{i, j} W_{jk}^{(2)2})$) where $\\lambda$ and $W_{ij}^{(1)}$ are considered constants."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We can repeatedly apply chain rule as discussed above to obtain the derivatives of the loss with respect to all the parameters of the model $\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$.\n",
    "Derive the expressions for the derivatives of the regularized loss (in Q1) w.r.t $W^{(1)}$, $b^{(1)}$, $b^{(2)}$ now.\n",
    "(please write your answer in the block below, or attach an image in the same cell, 6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{\\partial \\tilde{J}}{\\partial b^{(2)}} &= \\frac{\\partial \\tilde{J}}{\\partial z^{(3)}}*\\frac{\\partial z^{(3)}}{\\partial b^{(2)}} &= \\sum_k \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta)\\\\\n",
    "    \\frac{\\partial \\tilde{J}}{\\partial W^{(1)}} &=\\frac{\\partial J}{\\partial z^{(3)}}*\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}*\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}*\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} + \\frac{regLoss}{\\partial W^{(1)}} &= \\frac{1}{N} \\left[ \\left( (\\psi(z^{(3)}) - \\Delta) \\cdot W^{(2)} \\right) \\odot \\mathbb{1}_{z^{(2)} > 0} \\right] a^{(1)T} + 2\\lambda W^{(1)}\\\\\n",
    "    \\frac{\\partial \\tilde{J}}{\\partial b^{(1)}} &= \\frac{\\partial \\tilde{J}}{\\partial z^{(3)}}*\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}*\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}*\\frac{\\partial z^{(2)}}{\\partial b^{(1)}} &= \\sum_k \\frac{1}{N} \\left[ \\left( (\\psi(z^{(3)}) - \\Delta) \\cdot W^{(2)} \\right) \\odot \\mathbb{1}_{z^{(2)} > 0} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\odot$ is used to refer to element-wise multiplication while $\\cdot$ is referred for matrix multiplication."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Using the expressions you obtained for the derivatives of the loss w.r.t model parameters, implement the back-propagation algorithm. Run the code and verify that the gradients you obtained are correct using numerical gradients (already\n",
    "implemented in the code). The maximum relative error between the gradients you compute and the numerical gradients should be less than 1e-8 for all parameters.\n",
    "(5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 3.440708e-09\n",
      "b2 max relative error: 3.865039e-11\n",
      "b1 max relative error: 1.125423e-09\n",
      "W1 max relative error: 3.669858e-09\n"
     ]
    }
   ],
   "source": [
    "net_v3 = module_twolayernet.TwoLayerNetv3(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
    "loss, grads = net_v3.back_propagation(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net_v3.back_propagation(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net_v3.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Stochastic gradient descent training (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the backpropagation algorithm for computing the parameter gradients and have verified that it indeed gives the correct gradient. We are now ready to train the network. We solve Eq.15 with the stochastic gradient descent.\n",
    "\n",
    "Typically neural networks are large and are trained with millions of data\n",
    "points. It is thus often infeasible to compute the gradient $\\nabla_\\theta\n",
    "\\tilde{J}(\\theta)$ that requires the accumulation of the gradient over the\n",
    "entire training set. Stochastic gradient descent addresses this problem by\n",
    "simply accumulating the gradient over a small random subset of the training\n",
    "samples (minibatch) at each iteration. Specifically, the algorithm is as\n",
    "follows,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failed to load the image from ./data/exercise-2/alg1.png You can view it manually](./data/exercise-2/alg1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the gradient $\\nabla_\\theta \\tilde{J}(\\theta,\\{(X^\\prime_j,y^\\prime_j)\\}_{j=1}^B)$ is computed only on the current randomly sampled batch.\n",
    "\n",
    "Intuitively, $v = -\\nabla_\\theta \\tilde{J}(\\theta^{(t-1)})$ gives the direction\n",
    "to which the loss $\\tilde{J}$ decreases the most (locally), and therefore we\n",
    "follow that direction by updating the parameters towards that direction\n",
    "$\\theta^{(t)} = \\theta^{(t-1)} + v$. \n",
    "\n",
    "a) Implement the stochastic gradient descent algorithm and run the training on the toy data. Your model\n",
    "should be able to  obtain loss <= 0.02 on the training set and the training\n",
    "curve should look similar to the one shown in figure 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failed to load the image from ./data/exercise-2/fig2.png Please view it yourself](./data/exercise-2/fig2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.017149607938732048\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net_v4 = module_twolayernet.TwoLayerNetv4(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
    "stats = net_v4.train(X, y, X, y,\n",
    "                    learning_rate=1e-1, reg=5e-6,\n",
    "                    num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We are now ready to train our model on a real image dataset. For this, we will use\n",
    "the CIFAR-10 dataset.  Since the images are of size $32\\times 32$ pixels with 3\n",
    "color channels, this gives us 3072 input layer units, represented by a vector\n",
    "$x\\in\\mathbb{R}^{3072}$. The code to load the data and train the model is provided with\n",
    "some default hyperparameters. With default\n",
    "hyperparameters, if previous questions have been done correctly, you should get\n",
    "a validation set accuracy of about 29\\%. This is very poor.\n",
    "Your task is to debug the model training and come up with better hyperparameters\n",
    "to improve the performance on the validation set.\n",
    "Visualize the training and validation performance curves to help with this analysis.\n",
    "There are several pointers provided in the comments to \n",
    "help you understand why the network might be underperforming.\n",
    "Once you have tuned your hyperparameters, and get validation accuracy greater\n",
    "than 48\\% run your best model on the test set once and report the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download CIFAR-10 using this link: <http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>**\n",
    "<br/>\n",
    "**Decompress the downloaded dataset, and put the `cifar-10-batches-py` folder in the folder `data/exercise-2`**\n",
    "\n",
    "You can also try the next two commented lines (simply uncomment them and run the cell). The commands are tested on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 10:29:03--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170498071 (163M) [application/x-gzip]\n",
      "Saving to: ‘cifar-10-python.tar.gz’\n",
      "\n",
      "cifar-10-python.tar 100%[===================>] 162,60M  25,2MB/s    in 7,2s    \n",
      "\n",
      "2025-05-05 10:29:10 (22,5 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xzf cifar-10-python.tar.gz -C ./data/exercise-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import show_net_weights\n",
    "from utils.data_utils import get_CIFAR10_data\n",
    "from utils.vis_utils import visualize_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data\n",
    "# Now that you have implemented a two-layer network that passes\n",
    "# gradient checks and works on toy data, it's time to load up our favorite\n",
    "# CIFAR-10 data so we can use it to train a classifier on a real dataset.\n",
    "# Invoke the get_CIFAR10_data function to get our data.\n",
    "\n",
    "# Load the raw CIFAR-10 data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = \\\n",
    "    get_CIFAR10_data(data_dir='./data/exercise-2/cifar-10-batches-py')\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "# Visualize some images to get a feel for the data\n",
    "plt.imshow(visualize_grid(X_train[:100, :].reshape(100, 32,32, 3), padding=3).astype('uint8'))\n",
    "plt.gca().axis('off')\n",
    "plt.show()\n",
    "\n",
    "######################## Train a network ########################\n",
    "# To train our network we will use SGD. In addition, we will\n",
    "# adjust the learning rate with an exponential learning rate schedule as\n",
    "# optimization proceeds; after each epoch, we will reduce the learning rate by\n",
    "# multiplying it by a decay rate.\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = module_twolayernet.TwoLayerNetv4(input_size=input_size, hidden_size=hidden_size, output_size=num_classes)\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n",
    "######################## Debug the training ##################################\n",
    "# With the default parameters we provided above, you should get a validation\n",
    "# accuracy of about 0.29 on the validation set. This isn't very good.\n",
    "#\n",
    "# One strategy for getting insight into what's wrong is to plot the loss\n",
    "# function and the accuracies on the training and validation sets during\n",
    "# optimization.\n",
    "# Plot the loss function and train / validation accuracies\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(2*5, 5*1), dpi=100)\n",
    "axes[0].plot(stats['loss_history'])\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(stats['train_acc_history'], label='train')\n",
    "axes[1].plot(stats['val_acc_history'], label='val')\n",
    "axes[1].set_title('Classification accuracy history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Classification accuracy')\n",
    "axes[1].legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Another strategy is to visualize the weights that were learned in the first\n",
    "# layer of the network. In most neural networks trained on visual data, the\n",
    "# first layer weights typically show some visible structure when visualized.\n",
    "show_net_weights(net)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the hyper-parameters over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **What's wrong?**. Looking at the visualizations above, we see that the loss\n",
    "# is decreasing more or less linearly, which seems to suggest that the learning\n",
    "# rate may be too low. Moreover, there is no gap between the training and\n",
    "# validation accuracy, suggesting that the model we used has low capacity, and\n",
    "# that we should increase its size. On the other hand, with a very large model\n",
    "# we would expect to see more overfitting, which would manifest itself as a\n",
    "# very large gap between the training and validation accuracy.\n",
    "#\n",
    "# **Tuning**. Tuning the hyperparameters and developing intuition for how they\n",
    "# affect the final performance is a large part of using Neural Networks, so we\n",
    "# want you to get a lot of practice. Below, you should experiment with\n",
    "# different values of the various hyperparameters, including hidden layer size,\n",
    "# learning rate, numer of training epochs, and regularization strength. You\n",
    "# might also consider tuning the learning rate decay, but you should be able to\n",
    "# get good performance using the default value.\n",
    "#\n",
    "# **Approximate results**. You should aim to achieve a classification\n",
    "# accuracy of greater than 48% on the validation set. Our best network gets\n",
    "# over 52% on the validation set.\n",
    "#\n",
    "# **Experiment**: You goal in this exercise is to get as good of a result on\n",
    "# CIFAR-10 as you can (52% could serve as a reference), with a fully-connected\n",
    "# Neural Network. Feel free implement your own techniques (e.g. PCA to reduce\n",
    "# dimensionality, or adding dropout, or adding features to the solver, etc.).\n",
    "\n",
    "# **Explain your hyperparameter tuning process in the report.**\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_net.                                                            #\n",
    "#                                                                               #\n",
    "# To help debug your network, it may help to use visualizations similar to the  #\n",
    "# ones we used above; these visualizations will have significant qualitative    #\n",
    "# differences from the ones we saw above for the poorly tuned network.          #\n",
    "#                                                                               #\n",
    "# Tweaking hyperparameters- by hand can be fun, but you might find it useful to #\n",
    "# write code to sweep through possible combinations of hyperparameters          #\n",
    "# For example you can write                                                     #\n",
    "#   for learning_rate in my_learning_rates:                                     #\n",
    "#     ...                                                                       #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# stats = best_net.train(X_train, y_train, X_val, y_val,\n",
    "#             num_iters=, batch_size=,\n",
    "#             learning_rate=, learning_rate_decay=,\n",
    "#             reg=, verbose=True)\n",
    "\n",
    "best_net = net # store your best model into this\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (best_net.predict(X_val) == y_val).mean()\n",
    "print(f'Validation accuracy: {val_acc*100 : 0.2f}')\n",
    "show_net_weights(best_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done experimenting, you should evaluate your final trained\n",
    "# network on the test set; you should get more than 48% accuracy on the test set.\n",
    "\n",
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print(f'Test accuracy: {test_acc*100:0.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Implement multi-layer perceptron using PyTorch library (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have implemented a two-layer network by explicitly writing down the expressions for the forward and backward computations and training algorithms using simple matrix multiplication primitives from the NumPy library.\n",
    "\n",
    "However there are many libraries available designed make experimenting with neural networks faster by abstracting away the details into re-usable modules. One such popular open-source library is PyTorch (https://pytorch.org/). In this final question we will use the PyTorch library to implement the same two-layer network we did before and train it on the Cifar-10 dataset. However, extending a two-layer network to a three or four layered one is a matter of changing two-three lines of code using PyTorch. We will take advantage of this to experiment with deeper networks to improve the performance on the CIFAR-10 classification.\n",
    "\n",
    "To install the pytorch library follow the instruction in\n",
    "https://pytorch.org/get-started/locally/ . If you have access to a Graphics Processing\n",
    "Unit (GPU), you can install the gpu verison and run the exercise on GPU for faster run\n",
    "times. If not, you can install the cpu version (select cuda version None) and run on the\n",
    "cpu. Having gpu access is not necessary to complete the exercise.  There are good tutorials\n",
    "for getting started with pytorch on their website (https://pytorch.org/tutorials/).\n",
    "\n",
    "a) Complete the code to implement a multi-layer perceptron network in the class\n",
    "`MultiLayerPerceptron`. This includes instantiating the\n",
    "required layers from `torch.nn` and writing the code for forward pass. Initially you \n",
    "should write the code for the same two-layer network we have seen before.\n",
    "(3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "train = True\n",
    "drop_prob=0.1\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "\n",
    "# This time we rely on torchvision.datasets classes, as they have already provide implementation\n",
    "# for many datasets.\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Create a transform object, which pre-processes every sample before returning.\n",
    "norm_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                    ]\n",
    "                )\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(\n",
    "                    root='./data/exercise-2/',\n",
    "                    train=True,\n",
    "                    transform=norm_transform,\n",
    "                    download=False # Since we already downloaded it in the previous questions\n",
    "                )\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "                    root='./data/exercise-2/',\n",
    "                    train=False,\n",
    "                    transform=norm_transform\n",
    "                )\n",
    "#-------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "# We use the Subset wrapper dataset, which takes a dataset object and certain indices\n",
    "# and returns a new Dataset object of only those specific samples. \n",
    "#-------------------------------------------------\n",
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Data loader\n",
    "# Data Loader takes a dataset object and returns samples in batches.\n",
    "# It's a Generator and hence we can simply apply a for-loop over it.\n",
    "# You can also ask it to create batches of specific size and configure it \n",
    "# whether to shuffle the data (useful for training). \n",
    "#-------------------------------------------------\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain why the following two lengths are different and how they relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader), len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# Q4: Implementing multi-layer perceptron in PyTorch\n",
    "#======================================================================================\n",
    "# So far we have implemented a two-layer network using numpy by explicitly\n",
    "# writing down the forward computation and deriving and implementing the\n",
    "# equations for backward computation. This process can be tedious to extend to\n",
    "# large network architectures\n",
    "#\n",
    "# Popular deep-learining libraries like PyTorch and Tensorflow allow us to\n",
    "# quickly implement complicated neural network architectures. They provide\n",
    "# pre-defined layers which can be used as building blocks to define our\n",
    "# network. They also enable automatic-differentiation, which allows us to\n",
    "# define only the forward pass and let the libraries perform back-propagation\n",
    "# using automatic differentiation.\n",
    "#\n",
    "# In this question we will implement a multi-layer perceptron using the PyTorch\n",
    "# library.  Please complete the code for the MultiLayerPerceptron, training and\n",
    "# evaluating the model. Once you can train the two layer model, experiment with\n",
    "# adding more layers and\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Fully connected neural network with one hidden layer\n",
    "#-------------------------------------------------\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, drop_prob):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        #################################################################################\n",
    "        # TODO: Initialize the modules required to implement the mlp with given layer      #\n",
    "        # configuration. input_size --> hidden_layers[0] --> hidden_layers[1] .... -->     #\n",
    "        # hidden_layers[-1] --> num_classes                                                #\n",
    "        # Make use of linear and relu layers from the torch.nn module                      #\n",
    "        # Your code should be flexible in the sense that the length of hidden_layers.      #\n",
    "        # can vary.                                                                        #\n",
    "        # You would need to flatten the input as all the layers are Fully-connected layers #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        layers = []\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Sequential module takes many modules as argument and \n",
    "        # at forward pass propagates the data through all of them\n",
    "        self.layers = nn.Sequential(*layers) \n",
    "\n",
    "    def forward(self, x):\n",
    "        ##################################################################################\n",
    "        # TODO: Implement the forward pass computations                                  #\n",
    "        # Note that you do not need to use the softmax operation at the end.             #\n",
    "        # Softmax is only required for the loss computation and the criterion used below #\n",
    "        # nn.CrossEntropyLoss() already integrates the softmax and the log loss together #\n",
    "        ##################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return None\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes, drop_prob).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Complete the code to train the network. Make use of the loss function `torch.nn.CrossEntropyLoss` to compute the loss and `loss.backward()` to compute the gradients. Once gradients are computed, `optimizer.step()` can be invoked to update the model. Your should be able to achieve similar performance ($>$ 48\\% accuracy on the test set) as in Q3. Report the final test accuracy you achieve with a two-layer network. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.apply(weights_init)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #################################################################################\n",
    "        # TODO: Implement the training code                                                 #\n",
    "        # 1. Pass the images to the model                                                   #\n",
    "        # 2. Compute the loss using the output and the labels.                              #\n",
    "        # 3. Compute gradients and update the model using the optimizer                     #\n",
    "        # Do not forget to reset the gradient of model weights to zero after every step     #\n",
    "        # Use examples in https://pytorch.org/tutorials/beginner/pytorch_with_examples.html #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}', end='\\r')\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'\\nTrain accuracy is: {(100 * correct / total) : 0.2f} %')\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ####################################################\n",
    "            # TODO: Implement the evaluation code              #\n",
    "            # 1. Pass the images to the model                  #\n",
    "            # 2. Get the most confident predicted class        #\n",
    "            ####################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            predicted = None\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Validataion accuracy is: {(100 * correct / total):0.2f} %')\n",
    "\n",
    "    \n",
    "##################################################################################\n",
    "# TODO: Now that you can train a simple two-layer MLP using above code, you can  #\n",
    "# easily experiment with adding more layers and different layer configurations   #\n",
    "# and let the pytorch library handle computing the gradients                     #\n",
    "#                                                                                #\n",
    "# Experiment with different number of layers (atleast from 2 to 5 layers) and    #\n",
    "# record the final validation accuracies Report your observations on how adding  #\n",
    "# more layers to the MLP affects its behavior. Try to improve the model          #\n",
    "# configuration using the validation performance as the guidance. You can        #\n",
    "# experiment with different activation layers available in torch.nn, adding      #\n",
    "# dropout layers, if you are interested. Use the best model on the validation    #\n",
    "# set, to evaluate the performance on the test set once and report it            #\n",
    "##################################################################################\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = None # torch.load()\n",
    "best_model_state_dict = torch.load('model.ckpt', map_location='cpu')\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "        # TODO: Implement the evaluation code              #\n",
    "        # 1. Pass the images to the model                  #\n",
    "        # 2. Get the most confident predicted class        #\n",
    "        ####################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        predicted = None\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} test images: {(100 * correct / total):0.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
